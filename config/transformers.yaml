# ------------ Transformers' Arguments -------------------
model:
    model_name_or_path: ~
    cache_dir: ./cache
    model_type: ~
    config_overrides: ~
    config_name: ~
    tokenizer_name: ~
    use_fast_tokenizer: True
    model_revision: main
    token: ~
    use_auth_token: ~
    trust_remote_code: True
    low_cpu_mem_usage: False

train:
    output_dir: ./output
    overwrite_output_dir: False
    do_train: True
    do_eval: True
    do_predict: False
    evaluation_strategy: ~
    prediction_loss_only: ~
    per_device_train_batch_size: ~
    per_device_eval_batch_size: ~
    gradient_accumulation_steps: 1
    eval_accumulation_steps: 1
    eval_delay: 0
    learning_rate: ~
    weight_decay: ~
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1e-08
    max_grad_norm: 1.0
    num_train_epochs: ~
    max_steps: -1
    lr_scheduler_type: ~
    warmup_ratio: 0.0
    warmup_steps: 0
    log_level: info
    log_level_replica: info
    log_on_each_node: True
    logging_dir: ./log
    logging_strategy: ~
    logging_first_step: False
    logging_steps: 500
    logging_nan_inf_filter: ~
    save_strategy: ~
    save_steps: 500
    save_total_limit: 1
    save_safetensors: False
    save_on_each_node: False
    use_cpu: False
    seed: 42
    data_seed: ~
    jit_mode_eval: False
    use_ipex: False
    bf16: False
    fp16: True
    fp16_opt_level: O1
    fp16_backend: auto
    half_precision_backend: auto
    bf16_full_eval: False
    fp16_full_eval: False
    tf32: ~
    local_rank: -1
    ddp_backend: ~
    tpu_num_cores: ~
    dataloader_drop_last: False
    eval_steps: 500
    dataloader_num_workers: 0
    past_index: -1
    run_name: LM4Rec
    disable_tqdm: False
    remove_unused_columns: False
    label_names: ~
    load_best_model_at_end: False
    metric_for_best_model: ~
    greater_is_better: ~
    ignore_data_skip: False
    fsdp: False
    fsdp_min_num_params: 0
    fsdp_config:
        min_num_params: 0
        xla: False
        xla_fsdp_grad_ckpt: False

    fsdp_transformer_layer_cls_to_wrap: ~
    deepspeed: ~
    label_smoothing_factor: 0.0
    debug: ''
    optim: adamw_torch
    optim_args: ~
    adafactor: False
    group_by_length: False
    length_column_name: ~
    report_to: ['tensorboard']
    ddp_find_unused_parameters: True
    ddp_bucket_cap_mb: ~
    ddp_broadcast_buffers: ~
    dataloader_pin_memory: True
    skip_memory_metrics: True
    use_legacy_prediction_loop: False
    push_to_hub: False
    resume_from_checkpoint: ~
    hub_model_id: ~
    hub_strategy: every_save
    hub_token: ~
    hub_private_repo: True
    hub_always_push: False
    gradient_checkpointing: False
    include_inputs_for_metrics: False
    auto_find_batch_size: False
    full_determinism: False
    torchdynamo: ~
    ray_scope: last
    ddp_timeout: 1800
    use_mps_device: False
    torch_compile: False
    torch_compile_backend: ~
    torch_compile_mode: ~
    include_tokens_per_second: False
    dispatch_batches: ~


data:
    dataset_name: ~
    dataset_config_name: ~
    train_file: ~
    validation_file: ~
    overwrite_cache: False
    validation_split_percentage: 5
    max_seq_length: 512
    preprocessing_num_workers: ~
    mlm_probability: 0.15
    line_by_line: False
    pad_to_max_length: False
    max_train_samples: ~
    max_eval_samples: ~
    streaming: False


early_stop:
    early_stopping_patience: 3
    early_stopping_threshold: 1e-6
