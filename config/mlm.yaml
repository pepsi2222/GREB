# ------------ Transformers' Arguments For MLM -------------------
model:
    model_name_or_path: ~
    cache_dir: ./mlm/cache

train:
    output_dir: ./mlm/
    num_train_epochs: 3
    lr_scheduler_type: linear
    per_device_train_batch_size: 64
    per_device_eval_batch_size: 64
    logging_nan_inf_filter: False
    label_names: ~
    metric_for_best_model: loss
    greater_is_better: False
    prediction_loss_only: True
    # load_best_model_at_end: True
    
    learning_rate: 5e-5
    weight_decay: 0.0

    overwrite_output_dir: True

    # logging_strategy: epoch
    # save_strategy: epoch
    # evaluation_strategy: epoch

    logging_strategy: steps
    save_strategy: steps
    evaluation_strategy: steps
    eval_steps: 500
    save_steps: 500
    logging_steps: 500

data:
    max_seq_length: 512
    line_by_line: True
    
mask_target_item: True
